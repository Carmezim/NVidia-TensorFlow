{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "%matplotlib nbagg\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from data_generator import get_batch, print_valid_characters\n",
    "from tensorflow.python.framework.ops import reset_default_graph\n",
    "\n",
    "import tf_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Recurrent neural networks are the natural type of neural network to use for sequential data i.e. time series analysis, translation, speech recognition, biological sequence analysis etc. Recurrent neural networks works by recursively applying the same operation at each time step of the data sequence and having layers that pass information from previous time step to the current. It can therefore naturally handle input of varying length. Recurrent networks can be used for several prediction tasks including: sequence-to-class, sequence tagging, and sequence-to-sequence predictions.\n",
    "\n",
    "In this exercise we'll implement a Encoder-Decoder RNN based on the GRU unit for a simple sequence to sequence translation task. This type of models have shown impressive performance in Neural Machine Translation and Image Caption generation. \n",
    "\n",
    "For more in depth background material on RNNs please see [Supervised Sequence Labelling with Recurrent\n",
    "Neural Networks](https://www.cs.toronto.edu/~graves/preprint.pdf) by Alex Graves\n",
    "\n",
    "We know that LSTMs and GRUs are difficult to understand. A very good non-mathematical introduction is [Chris Olahs blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/). (All the posts are nice and cover various topics within machine-learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder\n",
    "In the encoder-decoder structure one RNN (blue) encodes the input and a second RNN (red) calculates the target values. One essential step is to let the encoder and decoder communicate. In the simplest approach you use the last hidden state of the encoder to initialize the decoder. Other approaches lets the decoder attend to different parts of the encoded input at different timesteps in the decoding process. \n",
    "\n",
    "<img src=\"files/enc-dec.png\", width=400>\n",
    "\n",
    "In our implementation we use a RNN with gated recurrent units (GRU) as encoder. We then use the last hidden state of the encoder ($h^{enc}_T$) as input to the decoder which is also a GRU RNN. \n",
    "\n",
    "### RNNs in TensorFlow\n",
    "TensorFlow has implementations of LSTM and GRU units. Both implementations assume that the input from the tensor below has the shape **(batch_size, seq_len, num_features)**, unless you have `time\\_major=True`. In this excercise we will use the GRU unit since it only stores a single hidden value per neuron (LSTMs stores two) and is approximately twice as fast as the LSTM unit.\n",
    "\n",
    "As stated above we will implement a Encoder-Decoder model. The simplest way to do this is to encode the input sequence using the Encoder model. We will then use the last hidden state of the Encoder $h^{enc}_T$ as input to the decoder model which then uses this information (simply a fixed length vector of numbers) to produce the targets. There is (at least) two ways to input $h^{enc}_T$ into the decoder\n",
    "\n",
    "1. Repeatly use $h^{enc}_T$ as input to the Decoder at each decode time step, as well as the previously computed word\n",
    "2. Intialize the decoder using $h^{enc}_T$ and run the decoder without any inputs\n",
    "\n",
    "In this exercise we will follow the second approach because it's easier to implement. To do this need to create a tensorflow layer that takes $h^{enc}_T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "Since RNN models can be very slow to train on real large datasets we will generate some simpler training data for this exercise. The task for the RNN is simply to translate a string of letters spelling the numbers between 0-9 into the corresponding numbers i.e\n",
    "\n",
    "\"one two five\" --> \"125#\" (we use # as a special end-of-sequence character)\n",
    "\n",
    "To input the strings into the RNN model we translate the characters into a vector integers using a simple translation table (i.e. 'h'->16, 'o'-> 17 etc). The code below prints a few input/output pairs using the *get_batch* function which randomy produces the data.\n",
    "\n",
    "Do note; that as showed in the illustration above for input to the decoder the end-of-sequence tag is flipped, and used in the beginning instead of the end. This tag is known as start-of-sequence, but often the end-of-sequence tag is just reused for this purpose.\n",
    "\n",
    "In the data loader below you will see two targets, target input and target output. Where the input will be used to compute the translation and output used for the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input types: int32 int32 int32 int32 int32\n",
      "Number of valid characters: 27\n",
      "'0'=0,\t'1'=1,\t'2'=2,\t'3'=3,\t'4'=4,\t'5'=5,\t'6'=6,\t'7'=7,\t'8'=8,\t'9'=9,\t'#'=10,\t' '=11,\t'e'=12,\t'g'=13,\t'f'=14,\t'i'=15,\t'h'=16,\t'o'=17,\t'n'=18,\t's'=19,\t'r'=20,\t'u'=21,\t't'=22,\t'w'=23,\t'v'=24,\t'x'=25,\t'z'=26,\t\n",
      "None\n",
      "Stop/start character = #\n",
      "\n",
      "SAMPLE 0\n",
      "TEXT INPUTS:\t\t\tnine\n",
      "TEXT TARGETS INPUT:\t\t#9\n",
      "TEXT TARGETS OUTPUT:\t\t9#\n",
      "ENCODED INPUTS:\t\t\t[18 15 18 12  0  0  0  0  0]\n",
      "INPUTS SEQUENCE LENGTH:\t\t4\n",
      "ENCODED TARGETS INPUT:\t\t[10  9  0]\n",
      "ENCODED TARGETS OUTPUT:\t\t[ 9 10  0]\n",
      "TARGETS SEQUENCE LENGTH:\t2\n",
      "TARGETS MASK:\t\t\t[ 1.  1.  0.]\n",
      "\n",
      "SAMPLE 1\n",
      "TEXT INPUTS:\t\t\tsix zero\n",
      "TEXT TARGETS INPUT:\t\t#60\n",
      "TEXT TARGETS OUTPUT:\t\t60#\n",
      "ENCODED INPUTS:\t\t\t[19 15 25 11 26 12 20 17  0]\n",
      "INPUTS SEQUENCE LENGTH:\t\t8\n",
      "ENCODED TARGETS INPUT:\t\t[10  6  0]\n",
      "ENCODED TARGETS OUTPUT:\t\t[ 6  0 10]\n",
      "TARGETS SEQUENCE LENGTH:\t3\n",
      "TARGETS MASK:\t\t\t[ 1.  1.  1.]\n",
      "\n",
      "SAMPLE 2\n",
      "TEXT INPUTS:\t\t\teight one\n",
      "TEXT TARGETS INPUT:\t\t#81\n",
      "TEXT TARGETS OUTPUT:\t\t81#\n",
      "ENCODED INPUTS:\t\t\t[12 15 13 16 22 11 17 18 12]\n",
      "INPUTS SEQUENCE LENGTH:\t\t9\n",
      "ENCODED TARGETS INPUT:\t\t[10  8  1]\n",
      "ENCODED TARGETS OUTPUT:\t\t[ 8  1 10]\n",
      "TARGETS SEQUENCE LENGTH:\t3\n",
      "TARGETS MASK:\t\t\t[ 1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 3\n",
    "inputs, inputs_seqlen, targets_in, targets_out, targets_seqlen, targets_mask, \\\n",
    "text_inputs, text_targets_in, text_targets_out = \\\n",
    "    get_batch(batch_size=batch_size, max_digits=2, min_digits=1)\n",
    "\n",
    "print \"input types:\", inputs.dtype, inputs_seqlen.dtype, targets_in.dtype, targets_out.dtype, targets_seqlen.dtype\n",
    "print print_valid_characters()\n",
    "print \"Stop/start character = #\"\n",
    "\n",
    "for i in range(batch_size):\n",
    "    print \"\\nSAMPLE\",i\n",
    "    print \"TEXT INPUTS:\\t\\t\\t\", text_inputs[i]\n",
    "    print \"TEXT TARGETS INPUT:\\t\\t\", text_targets_in[i]\n",
    "    print \"TEXT TARGETS OUTPUT:\\t\\t\", text_targets_out[i]\n",
    "    print \"ENCODED INPUTS:\\t\\t\\t\", inputs[i]\n",
    "    print \"INPUTS SEQUENCE LENGTH:\\t\\t\", inputs_seqlen[i]\n",
    "    print \"ENCODED TARGETS INPUT:\\t\\t\", targets_in[i]\n",
    "    print \"ENCODED TARGETS OUTPUT:\\t\\t\", targets_out[i]\n",
    "    print \"TARGETS SEQUENCE LENGTH:\\t\", targets_seqlen[i]\n",
    "    print \"TARGETS MASK:\\t\\t\\t\", targets_mask[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Decoder model setup\n",
    "Below is the TensorFlow model definition. We use an embedding layer to go from integer representation to vector representation of the input.\n",
    "\n",
    "Note that we have made use of a custom decoder wrapper which can be found in `rnn.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# resetting the graph\n",
    "reset_default_graph()\n",
    "\n",
    "# Setting up hyperparameters and general configs\n",
    "MAX_DIGITS = 5\n",
    "MIN_DIGITS = 5\n",
    "NUM_INPUTS = 27\n",
    "NUM_OUTPUTS = 11 #(0-9 + '#')\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "# try various learning rates 1e-2 to 1e-5\n",
    "LEARNING_RATE = 0.005\n",
    "X_EMBEDDINGS = 8\n",
    "t_EMBEDDINGS = 8\n",
    "NUM_UNITS_ENC = 10\n",
    "NUM_UNITS_DEC = 10\n",
    "\n",
    "\n",
    "# Setting up placeholders, these are the tensors that we \"feed\" to our network\n",
    "Xs = tf.placeholder(tf.int32, shape=[None, None], name='X_input')\n",
    "ts_in = tf.placeholder(tf.int32, shape=[None, None], name='t_input_in')\n",
    "ts_out = tf.placeholder(tf.int32, shape=[None, None], name='t_input_out')\n",
    "X_len = tf.placeholder(tf.int32, shape=[None], name='X_len')\n",
    "t_len = tf.placeholder(tf.int32, shape=[None], name='X_len')\n",
    "t_mask = tf.placeholder(tf.float32, shape=[None, None], name='t_mask')\n",
    "\n",
    "# Building the model\n",
    "\n",
    "# first we build the embeddings to make our characters into dense, trainable vectors\n",
    "X_embeddings = tf.get_variable('X_embeddings', [NUM_INPUTS, X_EMBEDDINGS],\n",
    "                               initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "t_embeddings = tf.get_variable('t_embeddings', [NUM_OUTPUTS, t_EMBEDDINGS],\n",
    "                               initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "\n",
    "# setting up weights for computing the final output\n",
    "W_out = tf.get_variable('W_out', [NUM_UNITS_DEC, NUM_OUTPUTS])\n",
    "b_out = tf.get_variable('b_out', [NUM_OUTPUTS])\n",
    "\n",
    "X_embedded = tf.gather(X_embeddings, Xs, name='embed_X')\n",
    "t_embedded = tf.gather(t_embeddings, ts_in, name='embed_t')\n",
    "\n",
    "# forward encoding\n",
    "enc_cell = tf.nn.rnn_cell.GRUCell(NUM_UNITS_ENC)#python.ops.rnn_cell.GRUCell\n",
    "_, enc_state = tf.nn.dynamic_rnn(cell=enc_cell, inputs=X_embedded,\n",
    "                                 sequence_length=X_len, dtype=tf.float32)\n",
    "# use below incase TF's makes issues\n",
    "#enc_state, _ = tf_utils.encoder(X_embedded, X_len, 'encoder', NUM_UNITS_ENC)\n",
    "#\n",
    "#enc_state = tf.concat(1, [enc_state, enc_state])\n",
    "\n",
    "# decoding\n",
    "# note that we are using a wrapper for decoding here, this wrapper is hardcoded to only use GRU\n",
    "# check out tf_utils to see how you make your own decoder\n",
    "dec_out, valid_dec_out = tf_utils.decoder(enc_state, t_embedded, t_len, \n",
    "                                          NUM_UNITS_DEC, t_embeddings,\n",
    "                                          W_out, b_out)\n",
    "\n",
    "# reshaping to have [batch_size*seqlen, num_units]\n",
    "out_tensor = tf.reshape(dec_out, [-1, NUM_UNITS_DEC])\n",
    "valid_out_tensor = tf.reshape(valid_dec_out, [-1, NUM_UNITS_DEC])\n",
    "# computing output\n",
    "out_tensor = tf.matmul(out_tensor, W_out) + b_out\n",
    "valid_out_tensor = tf.matmul(valid_out_tensor, W_out) + b_out\n",
    "# reshaping back to sequence\n",
    "b_size = tf.shape(X_len)[0] # use a variable we know has batch_size in [0]\n",
    "seq_len = tf.shape(t_embedded)[1] # variable we know has sequence length in [1]\n",
    "num_out = tf.constant(NUM_OUTPUTS) # casting NUM_OUTPUTS to a tensor variable\n",
    "out_shape = tf.concat(0, [tf.expand_dims(b_size, 0),\n",
    "                          tf.expand_dims(seq_len, 0),\n",
    "                          tf.expand_dims(num_out, 0)])\n",
    "out_tensor = tf.reshape(out_tensor, out_shape)\n",
    "valid_out_tensor = tf.reshape(valid_out_tensor, out_shape)\n",
    "# handling shape loss\n",
    "#out_tensor.set_shape([None, None, NUM_OUTPUTS])\n",
    "y = out_tensor\n",
    "y_valid = valid_out_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_embeddings:0                           (27, 8)\n",
      "t_embeddings:0                           (11, 8)\n",
      "W_out:0                                  (10, 11)\n",
      "b_out:0                                  (11,)\n",
      "RNN/GRUCell/Gates/Linear/Matrix:0        (18, 20)\n",
      "RNN/GRUCell/Gates/Linear/Bias:0          (20,)\n",
      "RNN/GRUCell/Candidate/Linear/Matrix:0    (18, 10)\n",
      "RNN/GRUCell/Candidate/Linear/Bias:0      (10,)\n",
      "decoder/W_z_x:0                          (8, 10)\n",
      "decoder/W_z_h:0                          (10, 10)\n",
      "decoder/b_z:0                            (10,)\n",
      "decoder/W_r_x:0                          (8, 10)\n",
      "decoder/W_r_h:0                          (10, 10)\n",
      "decoder/b_r:0                            (10,)\n",
      "decoder/W_c_x:0                          (8, 10)\n",
      "decoder/W_c_h:0                          (10, 10)\n",
      "decoder/b_h:0                            (10,)\n"
     ]
    }
   ],
   "source": [
    "# print all the variable names and shapes\n",
    "for var in tf.all_variables():\n",
    "    s = var.name + \" \"*(40-len(var.name))\n",
    "    print s, var.value().get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the cost function, gradient clipping and accuracy\n",
    "Because the targets are categorical we use the cross entropy error.\n",
    "As the data is sequential we use the sequence to sequence cross entropy supplied in `tf_utils.py`.\n",
    "We use the Adam optimizer but you can experiment with the different optimizers implemented in [TensorFlow](https://www.tensorflow.org/versions/r0.10/api_docs/python/train.html#optimizers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n"
     ]
    }
   ],
   "source": [
    "def loss_and_acc(preds):\n",
    "    # sequence_loss_tensor is a modification of TensorFlow's own sequence_to_sequence_loss\n",
    "    # TensorFlow's seq2seq loss works with a 2D list instead of a 3D tensors\n",
    "    loss = tf_utils.sequence_loss_tensor(preds, ts_out, t_mask, NUM_OUTPUTS) # notice that we use ts_out here!\n",
    "    # if you want regularization\n",
    "    #reg_scale = 0.00001\n",
    "    #regularize = tf.contrib.layers.l2_regularizer(reg_scale)\n",
    "    #params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "    #reg_term = sum([regularize(param) for param in params])\n",
    "    #loss += reg_term\n",
    "    # calculate accuracy\n",
    "    argmax = tf.to_int32(tf.argmax(preds, 2))\n",
    "    correct = tf.to_float(tf.equal(argmax, ts_out)) * t_mask\n",
    "    accuracy = tf.reduce_sum(correct) / tf.reduce_sum(t_mask)\n",
    "    return loss, accuracy, argmax\n",
    "\n",
    "loss, accuracy, predictions = loss_and_acc(y)\n",
    "loss_valid, accuracy_valid, predictions_valid = loss_and_acc(y_valid)\n",
    "\n",
    "# use lobal step to keep track of our iterations\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "# pick optimizer, try momentum or adadelta\n",
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "# extract gradients for each variable\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "# add below for clipping by norm\n",
    "#gradients, variables = zip(*grads_and_vars)  # unzip list of tuples\n",
    "#clipped_gradients, global_norm = (\n",
    "#    tf.clip_by_global_norm(gradients, self.clip_norm) )\n",
    "#grads_and_vars = zip(clipped_gradients, variables)\n",
    "# apply gradients and make trainable function\n",
    "train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_embeddings:0                           (27, 8)\n",
      "t_embeddings:0                           (11, 8)\n",
      "W_out:0                                  (10, 11)\n",
      "b_out:0                                  (11,)\n",
      "RNN/GRUCell/Gates/Linear/Matrix:0        (18, 20)\n",
      "RNN/GRUCell/Gates/Linear/Bias:0          (20,)\n",
      "RNN/GRUCell/Candidate/Linear/Matrix:0    (18, 10)\n",
      "RNN/GRUCell/Candidate/Linear/Bias:0      (10,)\n",
      "decoder/W_z_x:0                          (8, 10)\n",
      "decoder/W_z_h:0                          (10, 10)\n",
      "decoder/b_z:0                            (10,)\n",
      "decoder/W_r_x:0                          (8, 10)\n",
      "decoder/W_r_h:0                          (10, 10)\n",
      "decoder/b_r:0                            (10,)\n",
      "decoder/W_c_x:0                          (8, 10)\n",
      "decoder/W_c_h:0                          (10, 10)\n",
      "decoder/b_h:0                            (10,)\n",
      "global_step:0                            ()\n",
      "beta1_power:0                            ()\n",
      "beta2_power:0                            ()\n",
      "X_embeddings/Adam:0                      (27, 8)\n",
      "X_embeddings/Adam_1:0                    (27, 8)\n",
      "t_embeddings/Adam:0                      (11, 8)\n",
      "t_embeddings/Adam_1:0                    (11, 8)\n",
      "W_out/Adam:0                             (10, 11)\n",
      "W_out/Adam_1:0                           (10, 11)\n",
      "b_out/Adam:0                             (11,)\n",
      "b_out/Adam_1:0                           (11,)\n",
      "RNN/GRUCell/Gates/Linear/Matrix/Adam:0   (18, 20)\n",
      "RNN/GRUCell/Gates/Linear/Matrix/Adam_1:0 (18, 20)\n",
      "RNN/GRUCell/Gates/Linear/Bias/Adam:0     (20,)\n",
      "RNN/GRUCell/Gates/Linear/Bias/Adam_1:0   (20,)\n",
      "RNN/GRUCell/Candidate/Linear/Matrix/Adam:0 (18, 10)\n",
      "RNN/GRUCell/Candidate/Linear/Matrix/Adam_1:0 (18, 10)\n",
      "RNN/GRUCell/Candidate/Linear/Bias/Adam:0 (10,)\n",
      "RNN/GRUCell/Candidate/Linear/Bias/Adam_1:0 (10,)\n",
      "decoder/W_z_x/Adam:0                     (8, 10)\n",
      "decoder/W_z_x/Adam_1:0                   (8, 10)\n",
      "decoder/W_z_h/Adam:0                     (10, 10)\n",
      "decoder/W_z_h/Adam_1:0                   (10, 10)\n",
      "decoder/b_z/Adam:0                       (10,)\n",
      "decoder/b_z/Adam_1:0                     (10,)\n",
      "decoder/W_r_x/Adam:0                     (8, 10)\n",
      "decoder/W_r_x/Adam_1:0                   (8, 10)\n",
      "decoder/W_r_h/Adam:0                     (10, 10)\n",
      "decoder/W_r_h/Adam_1:0                   (10, 10)\n",
      "decoder/b_r/Adam:0                       (10,)\n",
      "decoder/b_r/Adam_1:0                     (10,)\n",
      "decoder/W_c_x/Adam:0                     (8, 10)\n",
      "decoder/W_c_x/Adam_1:0                   (8, 10)\n",
      "decoder/W_c_h/Adam:0                     (10, 10)\n",
      "decoder/W_c_h/Adam_1:0                   (10, 10)\n",
      "decoder/b_h/Adam:0                       (10,)\n",
      "decoder/b_h/Adam_1:0                     (10,)\n"
     ]
    }
   ],
   "source": [
    "# print all the variable names and shapes\n",
    "# notice that we now have the optimizer Adam as well!\n",
    "for var in tf.all_variables():\n",
    "    s = var.name + \" \"*(40-len(var.name))\n",
    "    print s, var.value().get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SAMPLE 0\n",
      "TEXT INPUTS:\t\t\tthree eight\n",
      "TEXT TARGETS INPUT:\t\t#38\n",
      "\n",
      "SAMPLE 1\n",
      "TEXT INPUTS:\t\t\tfour zero one\n",
      "TEXT TARGETS INPUT:\t\t#401\n",
      "\n",
      "SAMPLE 2\n",
      "TEXT INPUTS:\t\t\tthree three one\n",
      "TEXT TARGETS INPUT:\t\t#331\n",
      "y (3, 4, 11)\n",
      "y_valid (3, 4, 11)\n"
     ]
    }
   ],
   "source": [
    "# as always, test the forward pass and initialize the tf.Session!\n",
    "# here is some dummy data\n",
    "batch_size=3\n",
    "inputs, inputs_seqlen, targets_in, targets_out, targets_seqlen, targets_mask, \\\n",
    "text_inputs, text_targets_in, text_targets_out = \\\n",
    "    get_batch(batch_size=batch_size, max_digits=7, min_digits=2)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    print \"\\nSAMPLE\",i\n",
    "    print \"TEXT INPUTS:\\t\\t\\t\", text_inputs[i]\n",
    "    print \"TEXT TARGETS INPUT:\\t\\t\", text_targets_in[i]\n",
    "\n",
    "# restricting memory usage, TensorFlow is greedy and will use all memory otherwise\n",
    "gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
    "# initialize the Session\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts))\n",
    "# test train part\n",
    "sess.run(tf.initialize_all_variables())\n",
    "feed_dict = {Xs: inputs, X_len: inputs_seqlen, ts_in: targets_in,\n",
    "             ts_out: targets_out, t_len: targets_seqlen}\n",
    "fetches = [y]\n",
    "res = sess.run(fetches=fetches, feed_dict=feed_dict)\n",
    "print \"y\", res[0].shape\n",
    "\n",
    "# test validation part\n",
    "fetches = [y_valid]\n",
    "res = sess.run(fetches=fetches, feed_dict=feed_dict)\n",
    "print \"y_valid\", res[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_val (5000, 29)\n",
      "t_out_val (5000, 6)\n"
     ]
    }
   ],
   "source": [
    "#Generate some validation data\n",
    "X_val, X_len_val, t_in_val, t_out_val, t_len_val, t_mask_val, \\\n",
    "text_inputs_val, text_targets_in_val, text_targets_out_val = \\\n",
    "    get_batch(batch_size=5000, max_digits=MAX_DIGITS,min_digits=MIN_DIGITS)\n",
    "print \"X_val\", X_val.shape\n",
    "print \"t_out_val\", t_out_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# setting up running parameters\n",
    "val_interval = 5000\n",
    "samples_to_process = 3e5\n",
    "samples_processed = 0\n",
    "samples_val = []\n",
    "costs, accs_val = [], []\n",
    "plt.figure()\n",
    "try:\n",
    "    while samples_processed < samples_to_process:\n",
    "        # load data\n",
    "        X_tr, X_len_tr, t_in_tr, t_out_tr, t_len_tr, t_mask_tr, \\\n",
    "        text_inputs_tr, text_targets_in_tr, text_targets_out_tr = \\\n",
    "            get_batch(batch_size=BATCH_SIZE,max_digits=MAX_DIGITS,min_digits=MIN_DIGITS)\n",
    "        # make fetches\n",
    "        fetches_tr = [train_op, loss, accuracy]\n",
    "        # set up feed dict\n",
    "        feed_dict_tr = {Xs: X_tr, X_len: X_len_tr, ts_in: t_in_tr,\n",
    "             ts_out: t_out_tr, t_len: t_len_tr, t_mask: t_mask_tr}\n",
    "        # run the model\n",
    "        res = tuple(sess.run(fetches=fetches_tr, feed_dict=feed_dict_tr))\n",
    "        _, batch_cost, batch_acc = res\n",
    "        costs += [batch_cost]\n",
    "        samples_processed += BATCH_SIZE\n",
    "        #if samples_processed % 1000 == 0: print batch_cost, batch_acc\n",
    "        #validation data\n",
    "        if samples_processed % val_interval == 0:\n",
    "            #print \"validating\"\n",
    "            fetches_val = [accuracy_valid, y_valid]\n",
    "            feed_dict_val = {Xs: X_val, X_len: X_len_val, ts_in: t_in_val,\n",
    "             ts_out: t_out_val, t_len: t_len_val, t_mask: t_mask_val}\n",
    "            res = tuple(sess.run(fetches=fetches_val, feed_dict=feed_dict_val))\n",
    "            acc_val, output_val = res\n",
    "            samples_val += [samples_processed]\n",
    "            accs_val += [acc_val]\n",
    "            plt.plot(samples_val, accs_val, 'g-')\n",
    "            plt.ylabel('Validation Accuracy', fontsize=15)\n",
    "            plt.xlabel('Processed samples', fontsize=15)\n",
    "            plt.title('', fontsize=20)\n",
    "            plt.grid('on')\n",
    "            plt.savefig(\"out.png\")\n",
    "            display.display(display.Image(filename=\"out.png\"))\n",
    "            display.clear_output(wait=True)\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot of validation accuracy for each target position\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(np.mean(np.argmax(output_val,axis=2)==t_out_val,axis=0))\n",
    "plt.ylabel('Accuracy', fontsize=15)\n",
    "plt.xlabel('Target position', fontsize=15)\n",
    "#plt.title('', fontsize=20)\n",
    "plt.grid('on')\n",
    "plt.show()\n",
    "#why do the plot look like this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises:\n",
    "\n",
    "1. The model has two GRU networks. The ```GRUEncoder``` and the ```GRUDecoder```.\n",
    "A GRU is parameterized by a update gate `z`, reset gate `r` and the cell `c`.\n",
    "Under normal circumstances, such as in the TensorFlow GRUCell implementation, these gates have been stacked for faster computation, but in the custom decoder each weight and bias are as described in the original [article for GRU](https://arxiv.org/abs/1406.1078).\n",
    "Thus we have the following weights and bias; ```{decoder/W_z_x:0, decoder/W_z_h:0, b_updategate, decoder/b_z:0, decoder/W_r_x:0, decoder/W_r_h:0, decoder/b_r:0, decoder/W_c_x:0, decoder/W_c_h:0, decoder/b_h:0}```.\n",
    "Try to explain the shape of ```decoder/W_z_x:0``` and ```decoder/W_z_h:0```. Why are they different? You can find the equations for the gru at: [GRU](http://lasagne.readthedocs.io/en/latest/modules/layers/recurrent.html#lasagne.layers.GRULayer). \n",
    "\n",
    "decoder/W_z_x:0 is responsible for the updates and has shape [8, 10] representing this GRU gate weights where '8'"
    "is the embeddings size and '10' the number of decoded outputs being passed to the hidden decoder/W_z_h:0 layer." 
    "\n",
    "2. The GRUunit is able to ignore the input and just copy the previous hidden state. In the begining of training this might be desireable behaviour because it helps the model learn long range dependencies. You can make the model ignore the input by modifying initial bias values. What bias would you modify and how would you modify it? Again you'll need to refer to the GRU equations:  [GRU](http://lasagne.readthedocs.io/en/latest/modules/layers/recurrent.html#lasagne.layers.GRULayer)\n",
    "Further, if you look into `tf_utils.py` and search for the `decoder(...)` function, you will see that the init for each weight and bias can be changed.\n",
    "\n",
    "3. Try setting MIN_DIGITS and MAX_DIGITS to 20\n",
    "\n",
    "4. What is the final validation performance? Why do you think it is not better? Comment on the accuracy for each position in of the output symbols?\n",
    "\n",
    "5. Why do you think the validation performance looks more \"jig-saw\" like compared to FFN and CNN models?\n",
    "\n",
    "6. In the example we stack a softmax layer on top of a Recurrent layer. In the code snippet below explain how we can do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reset_default_graph()\n",
    "\n",
    "bs_, seqlen_, numinputs_ = 16, 140, 40\n",
    "x_pl_ = tf.placeholder(tf.float32, [bs_, seqlen_, numinputs_])\n",
    "gru_cell_ = tf.nn.rnn_cell.GRUCell(10)\n",
    "l_gru_, gru_state_ = tf.nn.dynamic_rnn(gru_cell_, x_pl_, dtype=tf.float32)\n",
    "l_reshape_ = tf.reshape(l_gru_, [-1, 10])\n",
    "\n",
    "l_softmax_ = tf.contrib.layers.fully_connected(l_reshape_, 11, activation_fn=tf.nn.softmax)\n",
    "l_softmax_seq_ = tf.reshape(l_softmax_, [bs_, seqlen_, -1])\n",
    "\n",
    "print \"l_input_\", x_pl_.get_shape()\n",
    "print \"l_gru_\", l_gru_.get_shape()\n",
    "print \"l_reshape_\", l_reshape_.get_shape()\n",
    "print \"l_softmax_\", l_softmax_.get_shape()\n",
    "print \"l_softmax_seq_\", l_softmax_seq_.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Optional: You are interested in doing sentiment analysis on tweets, i.e classification as positive or negative. You decide read over the twitter seqeuence and use the last hidden state to do the classification. How can you modify the small network above to only outa single classification for network? Hints: look at the gru\\_state\\_ or the [tf.slice](https://www.tensorflow.org/versions/r0.10/api_docs/python/array_ops.html#slice) in the API.\n",
    "\n",
    "\n",
    "7. Optional: Bidirectional Encoder, Bidirectional Encoders are usually implemented by running a forward model and  a backward model (a forward model on a reversed sequence) separately and the concatenating them before parsing them on to the next layer. To reverse the sequence try looking at [tf.reverse_sequence](https://www.tensorflow.org/versions/r0.10/api_docs/python/array_ops.html#reverse_sequence)\n",
    "\n",
    "```\n",
    "enc_cell = tf.nn.rnn_cell.GRUCell(NUM_UNITS_ENC)\n",
    "_, enc_state = tf.nn.dynamic_rnn(cell=enc_cell, inputs=X_embedded,\n",
    "                                 sequence_length=X_len, dtype=tf.float32, scope=\"rnn_forward\")\n",
    "\n",
    "X_embedded_backwards = tf.reverse_sequence(X_embedded, tf.to_int64(X_len), 1)\n",
    "enc_cell_backwards = tf.nn.rnn_cell.GRUCell(NUM_UNITS_ENC)\n",
    "_, enc_state_backwards = tf.nn.dynamic_rnn(cell=enc_cell_backwards, inputs=X_embedded_backwards,\n",
    "                                 sequence_length=X_len, dtype=tf.float32, scope=\"rnn_backward\")\n",
    "\n",
    "enc_state = tf.concat(1, [enc_state, enc_state_backwards])\n",
    "```\n",
    "\n",
    "Note: you will need to double the NUM_UNITS_DEC, as it currently does not support different sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Decoder (LSTM)\n",
    "Soft attention for recurrent neural networks have recently attracted a lot of interest.\n",
    "These methods let the Decoder model selective focus on which part of the encoder sequence it will use for each decoded output symbol.\n",
    "This relieves the encoder from having to compress the input sequence into a fixed size vector representation passed on to the decoder.\n",
    "Secondly we can interrogate the decoder network about where it attends while producing the ouputs.\n",
    "below we'll implement an LSTM-decoder with selective attention and show that it significantly improves the performance of the toy translation task.\n",
    "\n",
    "The siminal attention paper is https://arxiv.org/pdf/1409.0473v7.pdf\n",
    "\n",
    "The principle of attention models is simple. \n",
    "\n",
    "1. Use the encoder to get the hidden represention $\\{h^1_e, ...h^n_e\\}$ for each position in the input sequence. \n",
    "2. for timestep $t$ in the decoder do for $m = 1...n$ : $a_m = f(h^m_e, h^d_t)$. Where f is a function returning a scalar value. \n",
    "3. You can then normalize the sequence of scalars $\\{a_1, ... a_n\\}$ to get probablities $\\{p_1, ... p_n\\}$.\n",
    "4. Weight each $h^e_t$ by its probablity $p_t$ and sum to get $h_{in}$.\n",
    "5. Use $h_{in}$ as an additional input to the decoder. $h_{in}$ is recalculated each time the decoder is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# resetting the graph\n",
    "reset_default_graph()\n",
    "\n",
    "# Setting up hyperparameters and general configs\n",
    "MAX_DIGITS = 10\n",
    "MIN_DIGITS = 10\n",
    "NUM_INPUTS = 27\n",
    "NUM_OUTPUTS = 11 #(0-9 + '#')\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "# try various learning rates 1e-2 to 1e-5\n",
    "LEARNING_RATE = 0.005\n",
    "X_EMBEDDINGS = 8\n",
    "t_EMBEDDINGS = 8\n",
    "NUM_UNITS_ENC = 10\n",
    "NUM_UNITS_DEC = 10\n",
    "NUM_UNITS_ATTN = 20\n",
    "\n",
    "\n",
    "# Setting up placeholders, these are the tensors that we \"feed\" to our network\n",
    "Xs = tf.placeholder(tf.int32, shape=[None, None], name='X_input')\n",
    "ts_in = tf.placeholder(tf.int32, shape=[None, None], name='t_input_in')\n",
    "ts_out = tf.placeholder(tf.int32, shape=[None, None], name='t_input_out')\n",
    "X_len = tf.placeholder(tf.int32, shape=[None], name='X_len')\n",
    "t_len = tf.placeholder(tf.int32, shape=[None], name='X_len')\n",
    "t_mask = tf.placeholder(tf.float32, shape=[None, None], name='t_mask')\n",
    "\n",
    "# Building the model\n",
    "\n",
    "# first we build the embeddings to make our characters into dense, trainable vectors\n",
    "X_embeddings = tf.get_variable('X_embeddings', [NUM_INPUTS, X_EMBEDDINGS],\n",
    "                               initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "t_embeddings = tf.get_variable('t_embeddings', [NUM_OUTPUTS, t_EMBEDDINGS],\n",
    "                               initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "\n",
    "# setting up weights for computing the final output\n",
    "W_out = tf.get_variable('W_out', [NUM_UNITS_DEC, NUM_OUTPUTS])\n",
    "b_out = tf.get_variable('b_out', [NUM_OUTPUTS])\n",
    "\n",
    "X_embedded = tf.gather(X_embeddings, Xs, name='embed_X')\n",
    "t_embedded = tf.gather(t_embeddings, ts_in, name='embed_t')\n",
    "\n",
    "# forward encoding\n",
    "enc_cell = tf.nn.rnn_cell.GRUCell(NUM_UNITS_ENC)#python.ops.rnn_cell.GRUCell\n",
    "enc_out, enc_state = tf.nn.dynamic_rnn(cell=enc_cell, inputs=X_embedded,\n",
    "                                 sequence_length=X_len, dtype=tf.float32)\n",
    "# use below in case TF's does not work as intended\n",
    "#enc_state, _ = tf_utils.encoder(X_embedded, X_len, 'encoder', NUM_UNITS_ENC)\n",
    "#\n",
    "#enc_state = tf.concat(1, [enc_state, enc_state])\n",
    "\n",
    "# decoding\n",
    "# note that we are using a wrapper for decoding here, this wrapper is hardcoded to only use GRU\n",
    "# check out tf_utils to see how you make your own decoder\n",
    "dec_out, dec_out_valid, alpha_valid = \\\n",
    "    tf_utils.attention_decoder(enc_out, X_len, enc_state, t_embedded, t_len,\n",
    "                               NUM_UNITS_DEC, NUM_UNITS_ATTN, t_embeddings,\n",
    "                               W_out, b_out)\n",
    "\n",
    "# reshaping to have [batch_size*seqlen, num_units]\n",
    "out_tensor = tf.reshape(dec_out, [-1, NUM_UNITS_DEC])\n",
    "out_tensor_valid = tf.reshape(dec_out_valid, [-1, NUM_UNITS_DEC])\n",
    "# computing output\n",
    "out_tensor = tf.matmul(out_tensor, W_out) + b_out\n",
    "out_tensor_valid = tf.matmul(out_tensor_valid, W_out) + b_out\n",
    "# reshaping back to sequence\n",
    "b_size = tf.shape(X_len)[0] # use a variable we know has batch_size in [0]\n",
    "seq_len = tf.shape(t_embedded)[1] # variable we know has sequence length in [1]\n",
    "num_out = tf.constant(NUM_OUTPUTS) # casting NUM_OUTPUTS to a tensor variable\n",
    "out_shape = tf.concat(0, [tf.expand_dims(b_size, 0),\n",
    "                          tf.expand_dims(seq_len, 0),\n",
    "                          tf.expand_dims(num_out, 0)])\n",
    "out_tensor = tf.reshape(out_tensor, out_shape)\n",
    "out_tensor_valid = tf.reshape(out_tensor_valid, out_shape)\n",
    "# handling shape loss\n",
    "#out_tensor.set_shape([None, None, NUM_OUTPUTS])\n",
    "y = out_tensor\n",
    "y_valid = out_tensor_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss_and_acc(preds):\n",
    "    # sequence_loss_tensor is a modification of TensorFlow's own sequence_to_sequence_loss\n",
    "    # TensorFlow's seq2seq loss works with a 2D list instead of a 3D tensors\n",
    "    loss = tf_utils.sequence_loss_tensor(preds, ts_out, t_mask, NUM_OUTPUTS) # notice that we use ts_out here!\n",
    "    # if you want regularization\n",
    "    reg_scale = 0.00001\n",
    "    regularize = tf.contrib.layers.l2_regularizer(reg_scale)\n",
    "    params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "    reg_term = sum([regularize(param) for param in params])\n",
    "    loss += reg_term\n",
    "    # calculate accuracy\n",
    "    argmax = tf.to_int32(tf.argmax(preds, 2))\n",
    "    correct = tf.to_float(tf.equal(argmax, ts_out)) * t_mask\n",
    "    accuracy = tf.reduce_sum(correct) / tf.reduce_sum(t_mask)\n",
    "    return loss, accuracy, argmax\n",
    "\n",
    "loss, accuracy, predictions = loss_and_acc(y)\n",
    "loss_valid, accuracy_valid, predictions_valid = loss_and_acc(y_valid)\n",
    "\n",
    "# use lobal step to keep track of our iterations\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "# pick optimizer, try momentum or adadelta\n",
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "# extract gradients for each variable\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "# add below for clipping by norm\n",
    "#gradients, variables = zip(*grads_and_vars)  # unzip list of tuples\n",
    "#clipped_gradients, global_norm = (\n",
    "#    tf.clip_by_global_norm(gradients, self.clip_norm) )\n",
    "#grads_and_vars = zip(clipped_gradients, variables)\n",
    "# apply gradients and make trainable function\n",
    "train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# as always, test the forward pass and start the tf.Session!\n",
    "# here is some dummy data\n",
    "batch_size = 3\n",
    "inputs, inputs_seqlen, targets_in, targets_out, targets_seqlen, targets_mask, \\\n",
    "text_inputs, text_targets_in, text_targets_out = \\\n",
    "    get_batch(batch_size=3, max_digits=7, min_digits=2)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    print \"\\nSAMPLE\",i\n",
    "    print \"TEXT INPUTS:\\t\\t\\t\", text_inputs[i]\n",
    "    print \"TEXT TARGETS INPUT:\\t\\t\", text_targets_in[i]\n",
    "\n",
    "# restricting memory usage, TensorFlow is greedy and will use all memory otherwise\n",
    "gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
    "# initialize the Session\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts))\n",
    "# test train part\n",
    "sess.run(tf.initialize_all_variables())\n",
    "feed_dict = {Xs: inputs, X_len: inputs_seqlen, ts_in: targets_in,\n",
    "             ts_out: targets_out, t_len: targets_seqlen}\n",
    "fetches = [y]\n",
    "res = sess.run(fetches=fetches, feed_dict=feed_dict)\n",
    "print \"y\", res[0].shape\n",
    "\n",
    "# test validation part\n",
    "fetches = [y_valid]\n",
    "res = sess.run(fetches=fetches, feed_dict=feed_dict)\n",
    "print \"y_valid\", res[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print all the variable names and shapes\n",
    "# notice that W_z is now packed, such that it contains both W_z_h and W_x_h, this is for optimization\n",
    "# further, we now have W_s, b_s. This is so NUM_UNITS_ENC and NUM_UNITS_DEC does not have to share shape ..!\n",
    "for var in tf.all_variables():\n",
    "    s = var.name + \" \"*(40-len(var.name))\n",
    "    print s, var.value().get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate some validation data\n",
    "X_val, X_len_val, t_in_val, t_out_val, t_len_val, t_mask_val, \\\n",
    "text_inputs_val, text_targets_in_val, text_targets_out_val = \\\n",
    "    get_batch(batch_size=5000, max_digits=MAX_DIGITS,min_digits=MIN_DIGITS)\n",
    "print \"X_val\", X_val.shape\n",
    "print \"t_out_val\", t_out_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NOTICE - THIS MIGHT TAKE UPTO 30 MINUTES ON CPU..!\n",
    "# setting up running parameters\n",
    "val_interval = 5000\n",
    "samples_to_process = 3e5\n",
    "samples_processed = 0\n",
    "samples_val = []\n",
    "costs, accs = [], []\n",
    "plt.figure()\n",
    "try:\n",
    "    while samples_processed < samples_to_process:\n",
    "        # load data\n",
    "        X_tr, X_len_tr, t_in_tr, t_out_tr, t_len_tr, t_mask_tr, \\\n",
    "        text_inputs_tr, text_targets_in_tr, text_targets_out_tr = \\\n",
    "            get_batch(batch_size=BATCH_SIZE,max_digits=MAX_DIGITS,min_digits=MIN_DIGITS)\n",
    "        # make fetches\n",
    "        fetches_tr = [train_op, loss, accuracy]\n",
    "        # set up feed dict\n",
    "        feed_dict_tr = {Xs: X_tr, X_len: X_len_tr, ts_in: t_in_tr,\n",
    "             ts_out: t_out_tr, t_len: t_len_tr, t_mask: t_mask_tr}\n",
    "        # run the model\n",
    "        res = tuple(sess.run(fetches=fetches_tr, feed_dict=feed_dict_tr))\n",
    "        _, batch_cost, batch_acc = res\n",
    "        costs += [batch_cost]\n",
    "        samples_processed += BATCH_SIZE\n",
    "        #if samples_processed % 1000 == 0: print batch_cost, batch_acc\n",
    "        #validation data\n",
    "        if samples_processed % val_interval == 0:\n",
    "            #print \"validating\"\n",
    "            fetches_val = [accuracy_valid, y_valid, alpha_valid]\n",
    "            feed_dict_val = {Xs: X_val, X_len: X_len_val, ts_in: t_in_val,\n",
    "             ts_out: t_out_val, t_len: t_len_val, t_mask: t_mask_val}\n",
    "            res = tuple(sess.run(fetches=fetches_val, feed_dict=feed_dict_val))\n",
    "            acc_val, output_val, alp_val = res\n",
    "            samples_val += [samples_processed]\n",
    "            accs += [acc_val]\n",
    "            plt.plot(samples_val, accs, 'b-')\n",
    "            plt.ylabel('Validation Accuracy', fontsize=15)\n",
    "            plt.xlabel('Processed samples', fontsize=15)\n",
    "            plt.title('', fontsize=20)\n",
    "            plt.grid('on')\n",
    "            plt.savefig(\"out_attention.png\")\n",
    "            display.display(display.Image(filename=\"out_attention.png\"))\n",
    "            display.clear_output(wait=True)\n",
    "# NOTICE - THIS MIGHT TAKE UPTO 30 MINUTES ON CPU..!\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot of validation accuracy for each target position\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(np.mean(np.argmax(output_val,axis=2)==t_out_val,axis=0))\n",
    "plt.ylabel('Accuracy', fontsize=15)\n",
    "plt.xlabel('Target position', fontsize=15)\n",
    "#plt.title('', fontsize=20)\n",
    "plt.grid('on')\n",
    "plt.show()\n",
    "#why do the plot look like this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### attention plot, try with different i = 1, 2, ..., 1000\n",
    "i = 42\n",
    "\n",
    "column_labels = map(str, list(t_out_val[i]))\n",
    "row_labels = map(str, (list(X_val[i])))\n",
    "data = alp_val[i]\n",
    "fig, ax = plt.subplots()\n",
    "heatmap = ax.pcolor(data, cmap=plt.cm.Blues)\n",
    "\n",
    "# put the major ticks at the middle of each cell\n",
    "ax.set_xticks(np.arange(data.shape[1])+0.5, minor=False)\n",
    "ax.set_yticks(np.arange(data.shape[0])+0.5, minor=False)\n",
    "\n",
    "# want a more natural, table-like display\n",
    "ax.invert_yaxis()\n",
    "ax.xaxis.tick_top()\n",
    "\n",
    "ax.set_xticklabels(row_labels, minor=False)\n",
    "ax.set_yticklabels(column_labels, minor=False)\n",
    "\n",
    "plt.ylabel('output', fontsize=15)\n",
    "plt.xlabel('Attention plot', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot of average attention weight as a function of the sequence position for each of \n",
    "#the 21 targets in the output sequence i.e. each line is the mean postion of the \n",
    "#attention for each target position.\n",
    "\n",
    "np.mean(alp_val, axis=0).shape\n",
    "plt.figure()\n",
    "plt.plot(np.mean(alp_val, axis=0).T)\n",
    "plt.ylabel('alpha', fontsize=15)\n",
    "plt.xlabel('Input Sequence position', fontsize=15)\n",
    "plt.title('Alpha weights', fontsize=20)\n",
    "plt.legend(map(str,range(1,22)), bbox_to_anchor=(1.125,1.0), fontsize=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Assignments for the attention decoder\n",
    "1. Explain what the attention plot show.\n",
    "2. Explain what the alphaweights show.\n",
    "3. Why are the alpha curve for the first digit narrow and peaked while later digits have alpha curves that are wider and less peaked?\n",
    "4. Why is attention a good idea for this problem? Can you think of other problems where attention is a good choice?\n",
    "5. Try setting MIN_DIGITS and MAX_DIGITS to 20\n",
    "6. Enable gradient clipping (under the loss codeblock)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
